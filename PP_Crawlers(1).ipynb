{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawlers\n",
    "\n",
    "**Our plan for today:**\n",
    "\n",
    "1. What is a crawler?\n",
    "2. How can we create a simple crawler?\n",
    "3. How to avoid being blocked?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a crawler?\n",
    "\n",
    "A crawler is a program that crowls across the webpages and collects information. Use it well: don't use it to download the data that the webpage creators don't want you to download; avoid getting your ID blocked; don't overload the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a simple crawler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "session = requests.session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'94.228.207.11'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = session.get('https://ru.wikipedia.org')\n",
    "response.headers['X-Client-IP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept-Ranges': 'bytes',\n",
      " 'Age': '825',\n",
      " 'Cache-Control': 'private, s-maxage=0, max-age=0, must-revalidate',\n",
      " 'Connection': 'keep-alive',\n",
      " 'Content-Encoding': 'gzip',\n",
      " 'Content-Language': 'ru',\n",
      " 'Content-Length': '26513',\n",
      " 'Content-Type': 'text/html; charset=UTF-8',\n",
      " 'Date': 'Tue, 06 Oct 2020 15:00:39 GMT',\n",
      " 'Last-Modified': 'Tue, 06 Oct 2020 15:00:32 GMT',\n",
      " 'NEL': '{ \"report_to\": \"wm_nel\", \"max_age\": 86400, \"failure_fraction\": 0.05, '\n",
      "        '\"success_fraction\": 0.0}',\n",
      " 'P3p': 'CP=\"See https://ru.wikipedia.org/wiki/Special:CentralAutoLogin/P3P '\n",
      "        'for more info.\"',\n",
      " 'Report-To': '{ \"group\": \"wm_nel\", \"max_age\": 86400, \"endpoints\": [{ \"url\": '\n",
      "              '\"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" '\n",
      "              '}] }',\n",
      " 'Server': 'mw2240.codfw.wmnet',\n",
      " 'Server-Timing': 'cache;desc=\"hit-front\"',\n",
      " 'Strict-Transport-Security': 'max-age=106384710; includeSubDomains; preload',\n",
      " 'Vary': 'Accept-Encoding,Cookie,Authorization',\n",
      " 'X-Cache': 'cp3060 miss, cp3052 hit/319',\n",
      " 'X-Cache-Status': 'hit-front',\n",
      " 'X-Client-IP': '94.228.207.11',\n",
      " 'X-Content-Type-Options': 'nosniff',\n",
      " 'X-Request-Id': 'd3cc8eb2-2760-4e6f-b351-6848c597e693'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dict(response.headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies of data collection\n",
    "\n",
    "\n",
    "Basically, crawlers are collecting webpages (their html) in cycles, or cycles of cycles. Some strategies of data collection:\n",
    "    \n",
    "**By navigation type**\n",
    "\n",
    "1. All the webpages have convenient numbers (\"https://ficbook.net/fanfiction/no_fandom/originals?p=2\"), usually, p=(a number) or page=(a number). Then, you just have to go through relevant numbers.\n",
    "2. Webpages are named somehow. Then, you need to collect the links to the relevant webpages and then go through them to collect the data.\n",
    "\n",
    "**By the speed of updating**\n",
    "\n",
    "1. If the website is updated slowly, you can first collect the links to the webpages and then go through them\n",
    "2. If the website is updated fast, you need to collect the data right after you got the link and then move on to a new web page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding getting blocked\n",
    "\n",
    "Btw, Wikipedia doesn't block downloads, can be used as a source of linguistic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A pause between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for _ in range(5):\n",
    "    response = session.get('https://ru.wikipedia.org')\n",
    "    print(response.headers['Date'])\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To present yourself as a well-respected browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.2 Safari/537.36'}\n"
     ]
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "\n",
    "headers = {'User-Agent': ua.random}\n",
    "print(headers)\n",
    "response = session.get('https://ru.wikipedia.org', headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A pause between requests (random time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for _ in range(5):\n",
    "    response = session.get('https://ru.wikipedia.org')\n",
    "    print(response.headers['Date'])\n",
    "    time.sleep(random.uniform(1.1, 5.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a proxy\n",
    "\n",
    "There are websites where you can get a proxy address, e.g., [http://www.gatherproxy.com](http://www.gatherproxy.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180.246.205.208\n"
     ]
    }
   ],
   "source": [
    "known_proxy_ip = '180.246.205.208:57648'\n",
    "proxy = {'http': known_proxy_ip, 'https': known_proxy_ip}\n",
    "response = session.get('https://ru.wikipedia.org', proxies=proxy)\n",
    "print(response.headers['X-Client-IP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example\n",
    "\n",
    "Let's download some news from the HSE website!\n",
    "\n",
    "1. The webpages have the following structure: \"https://www.hse.ru/news/page1.html\", we can loop through them.\n",
    "2. Let's extract the publication date, the title, a short description, the full text of the publication, tags.\n",
    "3. Put the data we collected into a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from html import unescape\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('hse_news.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS texts \n",
    "(id int PRIMARY KEY, hse_id text, pub_year int, pub_month int, \n",
    "pub_day int, title text, short_text text, full_text text)\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tags \n",
    "(id int PRIMARY KEY, tag_name text) \n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS text_to_tag \n",
    "(id INTEGER PRIMARY KEY AUTOINCREMENT, id_text int, id_tag int) \n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1. Finding the webpages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_number = 1\n",
    "url = f'https://www.hse.ru/news/page{page_number}.html'\n",
    "req = session.get(url, headers={'User-Agent': ua.random})\n",
    "page = req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = soup.find_all('div', {'class': 'post_first'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Стартовал прием заявок на первый Всероссийский чемпионат сочинений «Своими словами»'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = news[0].find('a').text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': '/news/admission/405119387.html',\n",
       " 'class': ['link', 'link_dark2', 'no-visited']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs = news[0].find('a').attrs\n",
    "attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/news/admission/405119387.html'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "href = news[0].find('a').attrs['href']\n",
    "href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'До 15 ноября ученики 9-11 классов могут зарегистрироваться на онлайн-этап чемпионата, который пройдет в формате мультимедийного теста в личном кабинете участника. Чемпионат сочинений – это не типичная олимпиада или проверка на грамотность в рамках школьной программы. Задания не требуют специальной подготовки по типовым заданиям. Участникам необходимо просто рассказать о том, что им интересно, своими словами.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_text = news[0].find('div', {'class': 'post__text'}).text\n",
    "short_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_day = news[0].find('div', {'class': 'post-meta__day'}).text\n",
    "pub_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'окт'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_month = news[0].find('div', {'class': 'post-meta__month'}).text\n",
    "pub_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_year = news[0].find('div', {'class': 'post-meta__year'}).text\n",
    "pub_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2. Learn how to parse the webpage of the news article**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.hse.ru/news/admission/405119387.html'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_one = 'http://www.hse.ru'+href\n",
    "url_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = session.get(url_one, headers={'User-Agent': ua.random})\n",
    "page = req.text\n",
    "\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Стартовал прием заявок на первый Всероссийский чемпионат сочинений «Своими словами»© Всероссийский чемпионат сочинений «Своими словами»До 15 ноября ученики 9-11 классов могут зарегистрироваться на онл'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text = soup.find('div', {'class': 'post__content'}).text\n",
    "full_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Стартовал прием заявок на первый Всероссийский чемпионат сочинений «Своими словами»© Всероссийский чемпионат сочинений «Своими словами»До 15 ноября ученики 9-11 классов могут зарегистрироваться на онл'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text = soup.find('div', {'class': 'post__content'}).text\n",
    "full_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['новое в ВШЭ', 'приглашение к участию', 'довузовская подготовка']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = soup.find('div', {'class': 'articleMeta'})\n",
    "\n",
    "tags = meta.find_all('a', {'class': 'tag'})\n",
    "tags = [t.text for t in tags]\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3. Reformulating the steps in terms of functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {\n",
    "    value: key+1\n",
    "    for key, value in enumerate(\n",
    "        ['янв', 'фев', 'мар', 'апр', 'мая', 'июн', 'июл', 'авг', 'сен', 'окт', 'ноя', 'дек']\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse the information from the webpage with a list of news articles (for one block):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_first_level_info(one_block):\n",
    "    block = {}\n",
    "    block['title'] = one_block.find('a').text\n",
    "    block['href'] = one_block.find('a').attrs['href']\n",
    "    block['short_text'] = one_block.find('div', {'class': 'post__text'}).text\n",
    "    block['pub_day'] = int(one_block.find('div', {'class': 'post-meta__day'}).text)\n",
    "    block['pub_month'] = months[one_block.find('div', {'class': 'post-meta__month'}).text]\n",
    "    block['pub_year'] = int(one_block.find('div', {'class': 'post-meta__year'}).text)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse the webpage of a news article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_second_level_info(block):\n",
    "    url_one = 'http://www.hse.ru' + block['href']\n",
    "    req = session.get(url_one, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    block['full_text'] = soup.find('div', {'class': 'post__content'}).text\n",
    "    meta = soup.find('div', {'class': 'articleMeta'})\n",
    "    tags = meta.find_all('a', {'class': 'tag'})\n",
    "    block['tags'] = [t.text for t in tags]     \n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_hse_id = re.compile('/([0-9]*?).html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nth_page(page_number):\n",
    "    url = f'https://www.hse.ru/news/page{page_number}.html'\n",
    "    req = session.get(url, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    news = soup.find_all('div', {'class': 'post'})\n",
    "    blocks = []\n",
    "    for n in news:\n",
    "        try:\n",
    "            blocks.append(parse_first_level_info(n))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    result = []\n",
    "    for b in blocks:\n",
    "        if b['href'].startswith('/'):\n",
    "            idx = regex_hse_id.findall(b['href'])[0]\n",
    "            if idx not in seen_news:\n",
    "                try:\n",
    "                    res = parse_second_level_info(b)\n",
    "                    res['hse_id'] = idx\n",
    "                    result.append(res)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            else:\n",
    "                print('Seen', b['href'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4. Putting the data into a database**\n",
    "\n",
    "We need to create a dictionary for tags, a set of the articles we have seen (to not duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(block):\n",
    "    tags = []\n",
    "    for tag in block['tags']:\n",
    "        if tag in db_tags:\n",
    "            tags.append(db_tags[tag])\n",
    "        else:\n",
    "            db_tags[tag] = len(db_tags) + 1 \n",
    "            cur.execute('INSERT INTO tags VALUES (?, ?)', (len(db_tags), tag))\n",
    "            conn.commit()\n",
    "            tags.append(db_tags[tag])\n",
    "    text_id = len(seen_news) + 1\n",
    "    cur.execute(\n",
    "        'INSERT INTO texts VALUES (?, ?, ?, ?, ?, ?, ?, ?)',\n",
    "        (text_id, block['hse_id'],\n",
    "         block['pub_year'], block['pub_month'], block['pub_day'],\n",
    "         block['title'], block['short_text'], block['full_text'])\n",
    "    )\n",
    "    tags = [(text_id, t) for t in tags]\n",
    "    cur.executemany(\n",
    "        'INSERT INTO text_to_tag (id_text, id_tag) VALUES (?, ?)',\n",
    "        tags\n",
    "    )\n",
    "    conn.commit()\n",
    "    seen_news.add(block['hse_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('hse_news.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute('SELECT tag_name, id FROM tags')\n",
    "\n",
    "db_tags = {}\n",
    "for name, idx in cur.fetchall():\n",
    "    db_tags[name] = idx\n",
    "\n",
    "cur.execute('SELECT hse_id FROM texts')\n",
    "seen_news = set(i[0] for i in cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(n_pages):\n",
    "    for i in tqdm(range(n_pages)):\n",
    "        blocks = get_nth_page(i+1)\n",
    "        for block in blocks:\n",
    "            write_to_db(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76a0ba2df144e64a480070b4a8a67c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen /news/edu/377605768.html\n",
      "Seen /news/admission/299677085.html\n",
      "Seen /news/edu/359798781.html\n",
      "Seen /news/edu/316414634.html\n",
      "Seen /news/edu/316411087.html\n",
      "Seen /news/edu/316331850.html\n",
      "Seen /news/admission/316321399.html\n",
      "Seen /news/edu/316184838.html\n",
      "Seen /news/edu/316181564.html\n",
      "Seen /news/edu/316171943.html\n",
      "Seen /news/edu/316085154.html\n",
      "Seen /news/edu/315366464.html\n",
      "Seen /news/admission/315357192.html\n",
      "Seen /news/edu/315353288.html\n",
      "Seen /news/life/315315228.html\n",
      "Seen /news/science/315206335.html\n",
      "Seen /news/expertise/315196601.html\n",
      "Seen /news/expertise/315184185.html\n",
      "Seen /news/expertise/315170202.html\n",
      "Seen /news/edu/315092890.html\n",
      "Seen /news/science/315081605.html\n",
      "Seen /news/community/315061797.html\n",
      "Seen /news/edu/314949862.html\n",
      "Seen /news/edu/314949875.html\n",
      "Seen /news/life/314934132.html\n",
      "Seen /news/science/314837885.html\n",
      "Seen /news/edu/314807712.html\n",
      "Seen /news/edu/314690414.html\n",
      "Seen /news/edu/314571117.html\n",
      "Seen /news/science/314384109.html\n",
      "Seen /news/science/314268319.html\n",
      "Seen /news/expertise/314138976.html\n",
      "Seen /news/edu/314132648.html\n",
      "Seen /news/science/314127600.html\n",
      "Seen /news/community/314107805.html\n",
      "Seen /news/edu/313263609.html\n",
      "Seen /news/edu/312808398.html\n",
      "Seen /news/edu/312546181.html\n",
      "Seen /news/edu/312386730.html\n",
      "Seen /news/science/312290445.html\n",
      "Seen /news/community/311982209.html\n",
      "Seen /news/science/311948131.html\n",
      "Seen /news/edu/311630253.html\n",
      "Seen /news/edu/311613915.html\n",
      "Seen /news/edu/311622874.html\n",
      "Seen /news/science/311550312.html\n",
      "Seen /news/edu/311536327.html\n",
      "Seen /news/life/310840029.html\n",
      "Seen /news/science/310836397.html\n",
      "Seen /news/310739455.html\n",
      "Seen /news/science/310732009.html\n",
      "Seen /news/edu/310732443.html\n",
      "Seen /news/expertise/310656474.html\n",
      "Seen /news/life/310654188.html\n",
      "Seen /news/edu/310054804.html\n",
      "Seen /news/community/309431310.html\n",
      "Seen /news/community/309370747.html\n",
      "Seen /news/admission/309354863.html\n",
      "Seen /news/edu/309356830.html\n",
      "Seen /news/edu/309353064.html\n",
      "Seen /news/edu/309248739.html\n",
      "Seen /news/edu/309144820.html\n",
      "Seen /news/edu/309131755.html\n",
      "Seen /news/edu/309058740.html\n",
      "Seen /news/science/309053802.html\n",
      "Seen /news/edu/309027309.html\n",
      "Seen /news/edu/308944619.html\n",
      "Seen /news/edu/308276109.html\n",
      "Seen /news/science/308211791.html\n",
      "Seen /news/edu/308200408.html\n",
      "Seen /news/308126518.html\n",
      "Seen /news/science/308122550.html\n",
      "Seen /news/expertise/308048719.html\n",
      "Seen /news/life/308038740.html\n",
      "Seen /news/community/307961768.html\n",
      "Seen /news/edu/307959377.html\n",
      "Seen /news/edu/307233013.html\n",
      "Seen /news/edu/307230429.html\n",
      "Seen /news/science/307217525.html\n",
      "Seen /news/edu/307157278.html\n",
      "Seen /news/edu/307142607.html\n",
      "Seen /news/edu/307082961.html\n",
      "Seen /news/edu/307081965.html\n",
      "Seen /news/community/307077549.html\n",
      "Seen /news/edu/307009583.html\n",
      "Seen /news/306997603.html\n",
      "Seen /news/science/306993204.html\n",
      "Seen /news/life/306928716.html\n",
      "Seen /news/science/306917338.html\n",
      "Seen /news/306244698.html\n",
      "Seen /news/life/306173624.html\n",
      "Seen /news/edu/306028434.html\n",
      "Seen /news/edu/306019457.html\n",
      "Seen /news/science/305941917.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_all(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(277, 'студенты'),\n",
       " (205, 'новое в ВШЭ'),\n",
       " (204, 'магистратура'),\n",
       " (201, 'идеи и опыт'),\n",
       " (201, 'репортаж о событии'),\n",
       " (195, 'исследования и аналитика'),\n",
       " (147, 'бакалавриат'),\n",
       " (145, 'достижения'),\n",
       " (131, 'приглашение к участию'),\n",
       " (128, 'онлайн-образование')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"\"\"\n",
    "SELECT count(text_to_tag.id) as cnt, tags.tag_name \n",
    "    FROM text_to_tag \n",
    "        JOIN tags ON tags.id = text_to_tag.id_tag \n",
    "            GROUP BY text_to_tag.id_tag \n",
    "            ORDER BY cnt DESC\n",
    "            LIMIT 10;\n",
    "\"\"\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(166, 9),\n",
       " (159, 4),\n",
       " (152, 7),\n",
       " (127, 8),\n",
       " (125, 6),\n",
       " (112, 10),\n",
       " (104, 5),\n",
       " (93, 3),\n",
       " (86, 12),\n",
       " (83, 11),\n",
       " (78, 2),\n",
       " (67, 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"\"\"\n",
    "SELECT count(pub_month) as cnt, pub_month\n",
    "    FROM texts\n",
    "        GROUP BY pub_month\n",
    "        ORDER BY cnt DESC;\n",
    "\"\"\")\n",
    "cur.fetchall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Краулеры\n",
    "\n",
    "**План**\n",
    "\n",
    "1. Что такое краулеры?\n",
    "2. Как написать простой краулер?\n",
    "3. Блокировки и способы их обхода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое краулеры?\n",
    "\n",
    "Краулеры - это боты/рпограммы, которые \"ползают\" по страницам сайта и собирают информацию. Все чаще использование таких программ запрещается правилами пользования сайтами, поэтому это формально нехорошо. Но так продолжают делать и это надо уметь. Запрещают по 2 основным причинам: не хотят делиться данными и боятся, что вы уроните сервер (если сайт маленький и сервер не очень, то это довольно легко). Поэтому нужно собирать данные аккуратно, чтобы вас а) не заблокировали по IP и б) вы не навредили серверу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как написать простой краулер?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "session = requests.session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'188.18.72.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = session.get('https://ru.wikipedia.org')\n",
    "response.headers['X-Client-IP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept-Ranges': 'bytes',\n",
      " 'Age': '3397',\n",
      " 'Backend-Timing': 'D=185045 t=1573383643718757',\n",
      " 'Cache-Control': 'private, s-maxage=0, max-age=0, must-revalidate',\n",
      " 'Connection': 'keep-alive',\n",
      " 'Content-Encoding': 'gzip',\n",
      " 'Content-Length': '26617',\n",
      " 'Content-Type': 'text/html; charset=UTF-8',\n",
      " 'Content-language': 'ru',\n",
      " 'Date': 'Sun, 10 Nov 2019 11:57:22 GMT',\n",
      " 'Last-Modified': 'Sun, 10 Nov 2019 11:00:27 GMT',\n",
      " 'P3P': 'CP=\"See https://ru.wikipedia.org/wiki/Special:CentralAutoLogin/P3P '\n",
      "        'for more info.\"',\n",
      " 'Server': 'mw1333.eqiad.wmnet',\n",
      " 'Server-Timing': 'cache;desc=\"hit-front\"',\n",
      " 'Strict-Transport-Security': 'max-age=106384710; includeSubDomains; preload',\n",
      " 'Vary': 'Accept-Encoding,Cookie,Authorization',\n",
      " 'X-Analytics': 'ns=0;page_id=4401;WMF-Last-Access=10-Nov-2019;WMF-Last-Access-Global=10-Nov-2019;https=1',\n",
      " 'X-Cache': 'cp1083 pass, cp3060 hit/5, cp3062 hit/1473',\n",
      " 'X-Cache-Status': 'hit-front',\n",
      " 'X-Client-IP': '188.18.72.7',\n",
      " 'X-Content-Type-Options': 'nosniff',\n",
      " 'X-Powered-By': 'PHP/7.2.22-1+0~20190902.26+debian9~1.gbpd64eb7+wmf1',\n",
      " 'X-Varnish': '1033826911, 388705147 384416680, 307640691 245377865'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dict(response.headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стратегии сбора данных\n",
    "\n",
    "\n",
    "По сути краулеры выполняют сбор страниц (их html) как мы это делали на прошлом занятии, но делают они это циклами (или циклами циклов). Можно выделить разные стратегии сбора данных:\n",
    "    \n",
    "**По типу навигации**\n",
    "\n",
    "1. Все страницы со ссылками имеют удобные номера (\"https://ficbook.net/fanfiction/no_fandom/originals?p=2\"), обычно просто p=(число) или page=(число). В этом случае вам нужно просто подставлять цифры\n",
    "2. Страницы называются как-то не структурированно (например, по названиям блоков). Тут нужно собирать ссылки на эти страницы и потом по ним ходить и собирать конечные странички.\n",
    "\n",
    "**По скорости обновления**\n",
    "\n",
    "1. Если сайт довольно статичный по контенту (медленно появляются и удаляются материалы), то можно сначал собрать ссылки, а потом по ним ходить\n",
    "2. Если сайт очень динамичный по контенту (например, объявления на крупном сайте), вам нужно при получении страничкии ссылок сразу их обходить, а потом переходить к следующей, потому что ко времени получения исчерпывающего списка ссылок по сайту многие будут уже удалены или недоступны\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блокировки и способы их обхода\n",
    "\n",
    "Для того, чтобы предотвратить автоматический сбор информации с некого сайта, применяются различные инструменты, которые определяют роботов и блокируют запросы с адресов, которые были классифицированы как роботы. Чтобы не заблокировали домашний/учебный ip, лучше сразу задуматься об этих мерах и предотвратить возможные проблемы. Кстати, Википедия не блокирует и можно спокойно скачивать без каких-либо проблем.\n",
    "\n",
    "Чтобы их обойти, можно попробовать несколько инструментов:\n",
    "1. time.sleep(x) - задержка между запросами, чтобы слишком большая скорость запросов не показалась подозрительной или ваши запросы не уронили сервер небольшого ресурса (например, региональной газеты)\n",
    "2. time.sleep(случайный промежуток времени) - это более хитрая версия, когда время задержки - это случайное число из некоторого отрезка (модуль random)\n",
    "3. изобразить браузер - при запросе отправляется информация о том, из какого приложения пришел запрос (например, Googlr Chrome), запросы сделанные из браузера больше похожи на человеческие, для этого нужно задать user-agent в параметрах (а его выбирать случайно с помощью fake_useragent)\n",
    "4. использовать прокси - существуют ресурсы с бесплатными списками открытых прокси, через которые можно пропускать ваш запрос и сервер будет думать, что запросы приходят из разных мест (anonymous и elite классы прокси)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пауза между запросами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun, 10 Nov 2019 11:57:22 GMT\n",
      "Sun, 10 Nov 2019 11:57:25 GMT\n",
      "Sun, 10 Nov 2019 11:57:28 GMT\n",
      "Sun, 10 Nov 2019 11:57:32 GMT\n",
      "Sun, 10 Nov 2019 11:57:35 GMT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for _ in range(5):\n",
    "    response = session.get('https://ru.wikipedia.org')\n",
    "    print(response.headers['Date'])\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Притвориться нормальным браузером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36'}\n"
     ]
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "\n",
    "headers = {'User-Agent': ua.random}\n",
    "print(headers)\n",
    "response = session.get('https://ru.wikipedia.org', headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пауза между запросами (случайное время)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun, 10 Nov 2019 11:57:38 GMT\n",
      "Sun, 10 Nov 2019 11:57:42 GMT\n",
      "Sun, 10 Nov 2019 11:57:47 GMT\n",
      "Sun, 10 Nov 2019 11:57:50 GMT\n",
      "Sun, 10 Nov 2019 11:57:55 GMT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for _ in range(5):\n",
    "    response = session.get('https://ru.wikipedia.org')\n",
    "    print(response.headers['Date'])\n",
    "    time.sleep(random.uniform(1.1, 5.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключение через прокси\n",
    "\n",
    "Адреса прокси можно взять со специальных сайтов, например, [http://www.gatherproxy.com](http://www.gatherproxy.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180.246.205.208\n"
     ]
    }
   ],
   "source": [
    "known_proxy_ip = '180.246.205.208:57648'\n",
    "proxy = {'http': known_proxy_ip, 'https': known_proxy_ip}\n",
    "response = session.get('https://ru.wikipedia.org', proxies=proxy)\n",
    "print(response.headers['X-Client-IP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры\n",
    "\n",
    "### Пример 1\n",
    "\n",
    "Давайте обкачаем немного новостей с сайта вышки.\n",
    "\n",
    "1. Страницы имеют вид \"https://www.hse.ru/news/page1.html\", поэтому можно просто идти циклом.\n",
    "2. Достанем дату публикации, заголовок, краткое описание (из станицы со списком новостей), текст полной статьи и метки (из самой страницы новости)\n",
    "3. Положим в базу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from html import unescape\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('hse_news.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS texts \n",
    "(id int PRIMARY KEY, hse_id text, pub_year int, pub_month int, \n",
    "pub_day int, title text, short_text text, full_text text)\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tags \n",
    "(id int PRIMARY KEY, tag_name text) \n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS text_to_tag \n",
    "(id INTEGER PRIMARY KEY AUTOINCREMENT, id_text int, id_tag int) \n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаг 1. Найти страницы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_number = 2\n",
    "url = f'https://www.hse.ru/news/page{page_number}.html'\n",
    "req = session.get(url, headers={'User-Agent': ua.random})\n",
    "page = req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = soup.find_all('div', {'class': 'post_first'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'«Куратор выявляет то, что “разлито” в современности, но еще не сформулировано»'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = news[0].find('a').text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': '/news/edu/316171943.html',\n",
       " 'class': ['link', 'link_dark2', 'no-visited']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs = news[0].find('a').attrs\n",
    "attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/news/edu/316171943.html'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "href = news[0].find('a').attrs['href']\n",
    "href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В этом году НИУ ВШЭ и Музей современного искусства «Гараж» открыли магистерскую программу «Практики кураторства в современном искусстве», которая готовит кураторов и координаторов выставок. Что представляет собой профессия куратора в искусстве, в интервью новостной службе рассказали преподаватели программы – куратор Музея «Гараж» Валентин Дьяконов и главный редактор портала «Артгид» Мария Кравцова.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_text = news[0].find('div', {'class': 'post__text'}).text\n",
    "short_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_day = news[0].find('div', {'class': 'post-meta__day'}).text\n",
    "pub_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ноя'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_month = news[0].find('div', {'class': 'post-meta__month'}).text\n",
    "pub_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_year = news[0].find('div', {'class': 'post-meta__year'}).text\n",
    "pub_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаг 2. Научиться парсить страничку самой новости**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.hse.ru/news/edu/316171943.html'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_one = 'http://www.hse.ru'+href\n",
    "url_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = session.get(url_one, headers={'User-Agent': ua.random})\n",
    "page = req.text\n",
    "\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'«Куратор выявляет то, что “разлито” в современности, но еще не сформулировано»В этом году НИУ ВШЭ и Музей современного искусства «Гараж» открыли магистерскую программу\\xa0«Практики кураторства в современ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text = soup.find('div', {'class': 'post__content'}).text\n",
    "full_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'«Куратор выявляет то, что “разлито” в современности, но еще не сформулировано»В этом году НИУ ВШЭ и Музей современного искусства «Гараж» открыли магистерскую программу\\xa0«Практики кураторства в современ'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text = soup.find('div', {'class': 'post__content'}).text\n",
    "full_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['студенты', 'магистратура', 'Практики кураторства в современном искусстве']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = soup.find('div', {'class': 'articleMeta'})\n",
    "\n",
    "tags = meta.find_all('a', {'class': 'tag'})\n",
    "tags = [t.text for t in tags]\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаг 3. Оформляем нормально в функции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {\n",
    "    value: key+1\n",
    "    for key, value in enumerate(\n",
    "        ['янв', 'фев', 'мар', 'апр', 'мая', 'июн', 'июл', 'авг', 'сен', 'окт', 'ноя', 'дек']\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсим информацию из страницы со списком новостей (блок одной новости)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_first_level_info(one_block):\n",
    "    block = {}\n",
    "    block['title'] = one_block.find('a').text\n",
    "    block['href'] = one_block.find('a').attrs['href']\n",
    "    block['short_text'] = one_block.find('div', {'class': 'post__text'}).text\n",
    "    block['pub_day'] = int(one_block.find('div', {'class': 'post-meta__day'}).text)\n",
    "    block['pub_month'] = months[one_block.find('div', {'class': 'post-meta__month'}).text]\n",
    "    block['pub_year'] = int(one_block.find('div', {'class': 'post-meta__year'}).text)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсим отдельную страницу новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_second_level_info(block):\n",
    "    url_one = 'http://www.hse.ru' + block['href']\n",
    "    req = session.get(url_one, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    block['full_text'] = soup.find('div', {'class': 'post__content'}).text\n",
    "    meta = soup.find('div', {'class': 'articleMeta'})\n",
    "    tags = meta.find_all('a', {'class': 'tag'})\n",
    "    block['tags'] = [t.text for t in tags]     \n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_hse_id = re.compile('/([0-9]*?).html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nth_page(page_number):\n",
    "    url = f'https://www.hse.ru/news/page{page_number}.html'\n",
    "    req = session.get(url, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    news = soup.find_all('div', {'class': 'post'})\n",
    "    blocks = []\n",
    "    for n in news:\n",
    "        try:\n",
    "            blocks.append(parse_first_level_info(n))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    result = []\n",
    "    for b in blocks:\n",
    "        if b['href'].startswith('/'):\n",
    "            idx = regex_hse_id.findall(b['href'])[0]\n",
    "            if idx not in seen_news:\n",
    "                try:\n",
    "                    res = parse_second_level_info(b)\n",
    "                    res['hse_id'] = idx\n",
    "                    result.append(res)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            else:\n",
    "                print('Seen', b['href'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаг 4. Пишем в базу**\n",
    "\n",
    "Надо завести словарь для тегов (сначала читаем из базы, а потом дозаписываем), множество виденных статей (чтобы при перезаупске не дублировать)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(block):\n",
    "    tags = []\n",
    "    for tag in block['tags']:\n",
    "        if tag in db_tags:\n",
    "            tags.append(db_tags[tag])\n",
    "        else:\n",
    "            db_tags[tag] = len(db_tags) + 1 \n",
    "            cur.execute('INSERT INTO tags VALUES (?, ?)', (len(db_tags), tag))\n",
    "            conn.commit()\n",
    "            tags.append(db_tags[tag])\n",
    "    text_id = len(seen_news) + 1\n",
    "    cur.execute(\n",
    "        'INSERT INTO texts VALUES (?, ?, ?, ?, ?, ?, ?, ?)',\n",
    "        (text_id, block['hse_id'],\n",
    "         block['pub_year'], block['pub_month'], block['pub_day'],\n",
    "         block['title'], block['short_text'], block['full_text'])\n",
    "    )\n",
    "    tags = [(text_id, t) for t in tags]\n",
    "    cur.executemany(\n",
    "        'INSERT INTO text_to_tag (id_text, id_tag) VALUES (?, ?)',\n",
    "        tags\n",
    "    )\n",
    "    conn.commit()\n",
    "    seen_news.add(block['hse_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('hse_news.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute('SELECT tag_name, id FROM tags')\n",
    "\n",
    "db_tags = {}\n",
    "for name, idx in cur.fetchall():\n",
    "    db_tags[name] = idx\n",
    "\n",
    "cur.execute('SELECT hse_id FROM texts')\n",
    "seen_news = set(i[0] for i in cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(n_pages):\n",
    "    for i in tqdm(range(n_pages)):\n",
    "        blocks = get_nth_page(i+1)\n",
    "        for block in blocks:\n",
    "            write_to_db(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd629bb4a60842dc82d03839a4748e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_all(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(96, 'магистратура'),\n",
       " (84, 'студенты'),\n",
       " (72, 'приемная кампания 2019'),\n",
       " (68, 'бакалавриат'),\n",
       " (67, 'репортаж о событии'),\n",
       " (51, 'достижения'),\n",
       " (50, 'выпускники'),\n",
       " (42, 'приглашение к участию'),\n",
       " (36, 'новое в ВШЭ'),\n",
       " (36, 'рейтинг вузов')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"\"\"\n",
    "SELECT count(text_to_tag.id) as cnt, tags.tag_name \n",
    "    FROM text_to_tag \n",
    "        JOIN tags ON tags.id = text_to_tag.id_tag \n",
    "            GROUP BY text_to_tag.id_tag \n",
    "            ORDER BY cnt DESC\n",
    "            LIMIT 10;\n",
    "\"\"\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(99, 10),\n",
       " (87, 9),\n",
       " (78, 7),\n",
       " (68, 4),\n",
       " (62, 8),\n",
       " (55, 11),\n",
       " (44, 5),\n",
       " (42, 6),\n",
       " (40, 3),\n",
       " (37, 12),\n",
       " (35, 2),\n",
       " (29, 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"\"\"\n",
    "SELECT count(pub_month) as cnt, pub_month\n",
    "    FROM texts\n",
    "        GROUP BY pub_month\n",
    "        ORDER BY cnt DESC;\n",
    "\"\"\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
